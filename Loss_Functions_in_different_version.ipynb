{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6UOwtRDD0axpWtxZf9OAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanleyLiangYork/Advance_NN_techniques/blob/main/Loss_Functions_in_different_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Squared Error (MSE) Loss\n",
        "Mean Squared Error (MSE) loss is a commonly used loss function in regression problems, where the goal is to predict a continuous variable. The loss is calculated as the average of the squared differences between the predicted and true values. The formula for MSE loss is:\n",
        "\\begin{align}\n",
        "\\text{MSE loss} =  \\frac{1}{n}*\\sum{(y_{pred}-y_{true})^{2}}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "*   n is the number of samples in the dataset\n",
        "*   y_pred is the predicted value of the target variable\n",
        "*   y_true is the true value of the target variable\n"
      ],
      "metadata": {
        "id": "MhLIL9Ib0jH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy version"
      ],
      "metadata": {
        "id": "SNjeWfFM22nZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJrLErQg0dcN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error (MSE) loss between predicted and true values.\n",
        "    \n",
        "    Args:\n",
        "    - y_pred: predicted values\n",
        "    - y_true: true values\n",
        "    \n",
        "    Returns:\n",
        "    - mse_loss: mean squared error loss\n",
        "    \"\"\"\n",
        "    n = len(y_true)\n",
        "    mse_loss = np.sum((y_pred - y_true) ** 2) / n\n",
        "    return mse_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow version"
      ],
      "metadata": {
        "id": "esKct-sZ3AKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error (MSE) loss between predicted and true values.\n",
        "    \n",
        "    Args:\n",
        "    - y_pred: predicted values\n",
        "    - y_true: true values\n",
        "    \n",
        "    Returns:\n",
        "    - mse_loss: mean squared error loss\n",
        "    \"\"\"\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    mse_loss = mse(y_true, y_pred)\n",
        "    return mse_loss"
      ],
      "metadata": {
        "id": "RwbF8IbQ0iym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch version"
      ],
      "metadata": {
        "id": "GtB2t5j03QrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error (MSE) loss between predicted and true values.\n",
        "    \n",
        "    Args:\n",
        "    - y_pred: predicted values\n",
        "    - y_true: true values\n",
        "    \n",
        "    Returns:\n",
        "    - mse_loss: mean squared error loss\n",
        "    \"\"\"\n",
        "    mse = torch.nn.MSELoss()\n",
        "    mse_loss = mse(y_pred, y_true)\n",
        "    return mse_loss"
      ],
      "metadata": {
        "id": "CVE3E20B3SFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Cross-Entropy Loss\n",
        "Binary Cross-Entropy loss, also known as log loss, is a common loss function used in binary classification problems. It measures the difference between the predicted probability distribution and the actual binary label distribution.\n",
        "\\begin{align}\n",
        "\\text{L}(y, \\hat{y}) =  -[y * log(\\hat{y})+(1-y)*log((1-\\hat{y}))]\n",
        "\\end{align}\n",
        "where y is the true binary label (0 or 1), $\\hat{y}$ is the predicted probability (ranging from 0 to 1), and log is the natural logarithm. <p>\n",
        "The first term of the equation calculates the loss when the true label is 1, and the second term calculates the loss when the true label is 0. The overall loss is the sum of both terms."
      ],
      "metadata": {
        "id": "y6pgzauF3dFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy version"
      ],
      "metadata": {
        "id": "8kfYJTii54Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# define true labels and predicted probabilities\n",
        "y_true = np.array([0, 1, 1, 0])\n",
        "y_pred = np.array([0.1, 0.9, 0.8, 0.3])\n",
        "\n",
        "# calculate the binary cross-entropy loss\n",
        "loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
        "\n",
        "# print the loss\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7AdP8NQ56fi",
        "outputId": "a4cd4d99-a17b-48d7-91ee-97361764c7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19763488164214868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow version"
      ],
      "metadata": {
        "id": "p1mFnnkP6yTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define true labels and predicted probabilities\n",
        "y_true = tf.constant([0, 1, 1, 0], dtype=tf.float16)\n",
        "y_pred = tf.constant([0.1, 0.9, 0.8, 0.3], dtype=tf.float16)\n",
        "\n",
        "# define the loss function\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# calculate the loss\n",
        "loss = bce_loss(y_true, y_pred)\n",
        "\n",
        "# print the loss\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z4z0lE66zwc",
        "outputId": "39717fd0-d745-43e2-a4de-5de471bfcbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.1978, shape=(), dtype=float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch version"
      ],
      "metadata": {
        "id": "idrJclXY9ycw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# define true labels and predicted probabilities\n",
        "y_true = torch.tensor([0, 1, 1, 0], dtype=torch.float32)\n",
        "y_pred = torch.tensor([0.1, 0.9, 0.8, 0.3], dtype=torch.float32)\n",
        "\n",
        "# define the loss function\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "\n",
        "# calculate the loss\n",
        "loss = bce_loss(y_pred, y_true)\n",
        "\n",
        "# print the loss\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv8Exp2F64bU",
        "outputId": "ad2ad3e6-4306-4993-cd17-e65130716fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1976)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weighted Binary Cross-Entropy Loss\n",
        "Weighted Binary Cross-Entropy loss is a variation of the binary cross-entropy loss that allows for assigning different weights to positive and negative examples. This can be useful when dealing with imbalanced datasets, where one class is significantly underrepresented compared to the other.\n",
        "\\begin{align}\n",
        "\\text{L}(y, \\hat{y}) =  -[w_{pos} * y * log(\\hat{y})+w_{neg}(1-y)*log((1-\\hat{y}))]\n",
        "\\end{align}\n",
        "The positive and negative weights can be chosen based on the relative importance of each class. For example, if the positive class is more important, a higher weight can be assigned to it. Similarly, if the negative class is more important, a higher weight can be assigned to it.\n",
        "When the predicted probability is close to the true label, the loss is low, and when the predicted probability is far from the true label, the loss is high. This loss function is commonly used in neural network models that use sigmoid activation functions in the output layer to predict binary labels."
      ],
      "metadata": {
        "id": "gUYWdmPps1Y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow version"
      ],
      "metadata": {
        "id": "qXKvkzu33KZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# define true labels and predicted probabilities\n",
        "y_true = tf.constant([0, 1, 1, 0], dtype=tf.float32)\n",
        "y_pred = tf.constant([0.1, 0.9, 0.8, 0.3], dtype=tf.float32)\n",
        "pos_weight = 1.5\n",
        "\n",
        "# cannot create an object - pos_weight assigned a weight to the positive class (i.e.y_ture=1), when pos_weight>1, it increase the impact of the recall\n",
        "# when pos_weight<1, it increase the impact of pocision\n",
        "weighted_bce = tf.nn.weighted_cross_entropy_with_logits(labels=y_true, logits=y_pred, pos_weight=1.5)\n",
        "\n",
        "print(weighted_bce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcS-aOu92KQK",
        "outputId": "19ebeeeb-2732-4dc9-c06d-d104cd58644f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.7443967 0.5117308 0.556651  0.8543553], shape=(4,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch version"
      ],
      "metadata": {
        "id": "X8KR0SBQ7HZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# pos_weight = 3*torch.ones([1]) # the positive case has 3X weight\n",
        "pos_weight = torch.tensor(3.0, dtype=torch.float32)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(weight=pos_weight)\n",
        "\n",
        "y_true = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "y_pred = torch.tensor([[0.1], [0.9], [0.8], [-0.05]], dtype=torch.float32)\n",
        "\n",
        "loss = criterion(y_pred, y_true)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNeW_Eai7JZt",
        "outputId": "08b8195b-d075-4895-857e-1ee627b08183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.5938)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Cross-Entropy Loss\n",
        "\n",
        "The categorical cross-entropy loss is a popular loss function used in multi-class classification problems. It measures the dissimilarity between the true labels and the predicted probabilities for each class.\n",
        "\\begin{align}\n",
        "\\text{L}(y, \\hat{y}) =  -\\frac{1}{n}*\\sum\\sum( Y* log\\hat{Y})\n",
        "\\end{align}\n",
        "\n",
        "where Y is a matrix of true labels in one-hot encoding format, $\\hat{Y}$ is a matrix of predicted probabilities for each class, N is the number of samples, and log represents the natural logarithm."
      ],
      "metadata": {
        "id": "ZBCQX--gGb47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy version"
      ],
      "metadata": {
        "id": "5O6q-wwBLXNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# define true labels and predicted probabilities as NumPy arrays\n",
        "y_true = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
        "y_pred = np.array([[0.8, 0.1, 0.1], [0.2, 0.3, 0.5], [0.1, 0.6, 0.3]])\n",
        "\n",
        "# calculate the loss\n",
        "loss = -1/len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n",
        "\n",
        "# print the loss\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epHqIVf0GDox",
        "outputId": "a25d0035-5a1a-4e6a-feac-8c0e480098e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7661057888493454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow version"
      ],
      "metadata": {
        "id": "CGzy1p9BLnze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define true labels and predicted probabilities as TensorFlow Tensors\n",
        "y_true = tf.constant([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
        "y_pred = tf.constant([[0.8, 0.1, 0.1], [0.2, 0.3, 0.5], [0.1, 0.6, 0.3]])\n",
        "\n",
        "# create the loss object\n",
        "cce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# calculate the loss\n",
        "loss = cce_loss(y_true, y_pred)\n",
        "\n",
        "# print the loss\n",
        "print(loss.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hMMWpUPLdqV",
        "outputId": "ef0a11fe-d28c-4fd5-8187-33de82743492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7661058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the CategoricalCrossentropy class handles the conversion of the true labels to one-hot encoding internally, so you don't need to do it explicitly. "
      ],
      "metadata": {
        "id": "cEmIavG2MDiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define true labels and predicted probabilities as TensorFlow Tensors\n",
        "y_true = tf.constant([1,2,0])\n",
        "y_pred = tf.constant([0.9, 1.8, -0.1])\n",
        "\n",
        "# create the loss object\n",
        "cce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# calculate the loss\n",
        "loss = cce_loss(y_true, y_pred)\n",
        "\n",
        "# print the loss\n",
        "print(loss.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BybI_341MGS4",
        "outputId": "29b17ce5-a5c6-4e3e-9db1-d501ce8c48f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7963214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch version"
      ],
      "metadata": {
        "id": "jp10cqI2LsY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# define true labels and predicted logits as PyTorch Tensors\n",
        "# the CrossEntropyLoss class combines the softmax activation function and the categorical cross-entropy loss into a single operation, so you don't need to apply softmax separately.\n",
        "y_true = torch.LongTensor([1, 2, 0]) # the true label must be integer, NOT one-hot encoding format\n",
        "y_logits = torch.Tensor([[0.8, 0.1, 0.1], [0.2, 0.3, 0.5], [0.1, 0.6, 0.3]])\n",
        "\n",
        "# create the loss object\n",
        "ce_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# calculate the loss\n",
        "loss = ce_loss(y_logits, y_true)\n",
        "\n",
        "# print the loss\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41UVx5RVLu-t",
        "outputId": "33dc3c9b-4ee0-4aad-e77c-466b87af0d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2276147603988647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Categorical Cross-Entropy Loss\n",
        "The sparse categorical cross-entropy loss is similar to the categorical cross-entropy loss, but it is used when the true labels are provided as integers rather than one-hot encoding.\n",
        "\\begin{align}\n",
        "\\text{L}(y, \\hat{y}) =  -\\frac{1}{n}*\\sum\\log (\\hat{Y}_{i})\n",
        "\\end{align} \n",
        "where $\\hat{Y}_{i}$ is the predicted probability for the true class label i for each sample, and N is the number of samples. <p>\n",
        "The sparse categorical cross-entropy loss uses integer labels directly. The true label for each sample is represented as a single integer value $i$ between 0 and $C-1$, where $C$ is the number of classes."
      ],
      "metadata": {
        "id": "Tk7S9qJcT65s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy version"
      ],
      "metadata": {
        "id": "wzh4apUCgRAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sparse_categorical_crossentropy(y_true, y_pred):\n",
        "    # convert true labels to one-hot encoding\n",
        "    y_true_onehot = np.zeros_like(y_pred)\n",
        "    y_true_onehot[np.arange(len(y_true)), y_true] = 1\n",
        "\n",
        "    # calculate loss\n",
        "    loss = -np.mean(np.sum(y_true_onehot * np.log(y_pred), axis=-1))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# define true labels as integers and predicted probabilities as an array\n",
        "y_true = np.array([1, 2, 0]) # as integer in (0, C-1)\n",
        "y_pred = np.array([[0.1, 0.8, 0.1], [0.3, 0.2, 0.5], [0.4, 0.3, 0.3]]) # as one-hot encoding\n",
        "\n",
        "loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HNUuxJWQ8Hx",
        "outputId": "9c044422-a6a9-484e-d956-9cf3f3039e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6108604879161034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow version"
      ],
      "metadata": {
        "id": "cJtz3-1-mT-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def sparse_categorical_crossentropy(y_true, y_pred):\n",
        "  # set from_logits to False to ensure that y_pred represents probabilities rather than logit values.\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
        "    return loss\n",
        "\n",
        "# define true labels as integers and predicted probabilities as a tensor\n",
        "y_true = tf.constant([1, 2, 0])\n",
        "y_pred = tf.constant([[0.1, 0.8, 0.1], [0.3, 0.2, 0.5], [0.4, 0.3, 0.3]])\n",
        "\n",
        "# calculate the loss\n",
        "loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "# print the loss\n",
        "print(loss.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo8v-HnkkuaV",
        "outputId": "2070b322-9536-4fc2-8083-49b7447892d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.22314355 0.6931472  0.91629076]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch version"
      ],
      "metadata": {
        "id": "F8HL_aNmn1Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def sparse_categorical_crossentropy(y_true, y_pred):\n",
        "    loss = F.cross_entropy(y_pred, y_true)\n",
        "    return loss\n",
        "\n",
        "# define true labels as integers and predicted logits as a tensor\n",
        "y_true = torch.tensor([1, 2, 0])\n",
        "y_pred = torch.tensor([[0.1, 0.8, 0.1], [0.3, 0.2, 0.5], [0.4, 0.3, 0.3]])\n",
        "\n",
        "# calculate the loss\n",
        "loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "# print the loss\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFGzRJA9lPrq",
        "outputId": "3d34cc16-d2a2-494e-b059-8748aba669a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.887542188167572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dice Loss\n",
        "Dice loss, also known as the Sørensen–Dice coefficient or F1 score, is a loss function used in image segmentation tasks to measure the overlap between the predicted segmentation and the ground truth. The Dice loss ranges from 0 to 1, where 0 indicates no overlap and 1 indicates perfect overlap.\n",
        "\\begin{align}\n",
        "\\text Dice Loss = 1 - \\frac{2 * intersection + smooth} {\\sum({prediction})^{2} + \\sum{ground truth + smooth}^{2}}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "n2h2R9XEqpKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy version"
      ],
      "metadata": {
        "id": "lBRgled2sLor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def dice_loss(y_true, y_pred, smooth=1e-5):\n",
        "  intersection = np.sum(y_true * y_pred, axis=(1,2,3))\n",
        "  sum_of_squares_pred = np.sum(np.square(y_pred), axis=(1,2,3))\n",
        "  sum_of_squares_true = np.sum(np.square(y_true), axis=(1,2,3))\n",
        "  dice = 1 - (2 * intersection + smooth) / (sum_of_squares_pred + sum_of_squares_true + smooth)\n",
        "  return dice"
      ],
      "metadata": {
        "id": "nu5bMetDpEYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow version"
      ],
      "metadata": {
        "id": "-C154pMGs47k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def dice_loss(y_true, y_pred, smooth=1e-5):\n",
        "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n",
        "    sum_of_squares_pred = tf.reduce_sum(tf.square(y_pred), axis=(1,2,3))\n",
        "    sum_of_squares_true = tf.reduce_sum(tf.square(y_true), axis=(1,2,3))\n",
        "    dice = 1 - (2 * intersection + smooth) / (sum_of_squares_pred + sum_of_squares_true + smooth)\n",
        "    return dice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmNNOlW_paaO",
        "outputId": "6d7098c9-1146-4a4c-ceb4-1f52377ad745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1.601713251907458e-15,\n",
              " 0.46899559358927834,\n",
              " 0.7219280948873594,\n",
              " 0.8812908992306898,\n",
              " 0.9709505944546657,\n",
              " 0.9999999999999971]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch version"
      ],
      "metadata": {
        "id": "qq_YuS7OtJCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def dice_loss(y_true, y_pred, smooth=1e-5):\n",
        "    intersection = torch.sum(y_true * y_pred, dim=(1,2,3))\n",
        "    sum_of_squares_pred = torch.sum(torch.square(y_pred), dim=(1,2,3))\n",
        "    sum_of_squares_true = torch.sum(torch.square(y_true), dim=(1,2,3))\n",
        "    dice = 1 - (2 * intersection + smooth) / (sum_of_squares_pred + sum_of_squares_true + smooth)\n",
        "    return dice"
      ],
      "metadata": {
        "id": "pXJ37z63tNGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assumes that y_true and y_pred are 4D tensors with dimensions (batch_size, num_classes, height, width)"
      ],
      "metadata": {
        "id": "Z85LyuXstbKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KL Divergence Loss\n",
        "KL (Kullback-Leibler) divergence loss is a measure of how different two probability distributions are from each other. In the context of machine learning, it is often used as a loss function to train models that generate new samples from a given distribution.\n",
        "\\begin{align}\n",
        "\\text KL(p||q)=\\sum{(p(x)*log(\\frac{p(x)}{q(x)})}\n",
        "\\end{align}\n",
        "where p represents the true distribution and q represents the predicted distribution. The KL divergence loss measures how well the predicted distribution matches the true distribution."
      ],
      "metadata": {
        "id": "mJUPgdvHtqkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy version"
      ],
      "metadata": {
        "id": "D3WDtgKVuca3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def kl_divergence_loss(p, q):\n",
        "    return np.sum(p * np.log(p / q))"
      ],
      "metadata": {
        "id": "TIsel7U9ueFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow version <p>\n",
        "p and q are TensorFlow tensors representing the true distribution and predicted distribution, respectively. The tf.keras.losses.KLDivergence() function is used to compute the KL divergence loss between p and q. The result is a scalar tensor that represents the loss value."
      ],
      "metadata": {
        "id": "t4cZkOYIu_D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define true distribution and predicted distribution\n",
        "p = tf.constant([0.2, 0.3, 0.5])\n",
        "q = tf.constant([0.4, 0.3, 0.3])\n",
        "\n",
        "kl_loss = tf.keras.losses.KLDivergence()\n",
        "loss = kl_loss(p, q)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UDf7pSAutB_",
        "outputId": "ac84ffcc-8839-45da-8ef4-051261d8dedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.11678335, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = tf.random.normal([224,224], 0, 255, tf.float32)\n",
        "q = tf.random.normal([224,224], 0, 255, tf.float32)\n",
        "kl_loss = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.AUTO)\n",
        "loss = kl_loss(p, q)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioZ6GvHRvd0h",
        "outputId": "ab9b4a4c-2316-406b-e0ca-7a92ae7c49ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(894.798, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch version"
      ],
      "metadata": {
        "id": "zu9pv0AhyF26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "p = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
        "q = F.softmax(torch.rand(3, 5), dim=1)\n",
        "loss = criterion(p,q)\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a_asFohyJD0",
        "outputId": "3deea704-ec96-43d0-95fe-fa196493a73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5797662138938904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxPzCLwr1V0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}